{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["mBGJfICz3VLV","_2GFbE5eKdbY","txkpQyqeNciA","Gslh62xvQM5M","a1eGS61uVoFu"],"authorship_tag":"ABX9TyMUS1323OSCoboqjd2VocRF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9fb54453e24245e3a0d902fc26893588":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6600b2b62c8542548e869b21d4662ce3","IPY_MODEL_9d8e8302650c4607958f1259bcb377d7","IPY_MODEL_c83ba7a776544b43afcf56411c6b70df"],"layout":"IPY_MODEL_c28421b41fc2401ab61765d8d5c51b4b"}},"6600b2b62c8542548e869b21d4662ce3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2e33e1d5881a4eddbd1174ed425094d9","placeholder":"​","style":"IPY_MODEL_517839cfb1c84f65abd85c84aa3885f3","value":"Downloading (…)okenizer_config.json: 100%"}},"9d8e8302650c4607958f1259bcb377d7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a0128bb590c422cb9bec0a444ad45f9","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f89d23bcd3f449499448a1401774272","value":28}},"c83ba7a776544b43afcf56411c6b70df":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1011359bad5b4767b75dd6d2a1fad09e","placeholder":"​","style":"IPY_MODEL_5a4bd41668e644a591f82c2e8b7b0eee","value":" 28.0/28.0 [00:00&lt;00:00, 213B/s]"}},"c28421b41fc2401ab61765d8d5c51b4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e33e1d5881a4eddbd1174ed425094d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"517839cfb1c84f65abd85c84aa3885f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a0128bb590c422cb9bec0a444ad45f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f89d23bcd3f449499448a1401774272":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1011359bad5b4767b75dd6d2a1fad09e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4bd41668e644a591f82c2e8b7b0eee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f4eb1ad73764165a5683cd9ec025fd7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9e5f0e03a8c84605ad8ecc470573163b","IPY_MODEL_aaba5645fb9143d58d74dfd89bec41a0","IPY_MODEL_560b5f223bea4d418026662ffff08eab"],"layout":"IPY_MODEL_fefbe13782c746e591ca915b345878c3"}},"9e5f0e03a8c84605ad8ecc470573163b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_719c6dd822e1478faa48fd829c594ee6","placeholder":"​","style":"IPY_MODEL_75371e2e728b4d91a907b144bffe26b8","value":"Downloading (…)lve/main/config.json: 100%"}},"aaba5645fb9143d58d74dfd89bec41a0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a258401e6172493b9cabca14075bcd69","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8f38f337b3f54d7bb7c4d8035eb444f7","value":570}},"560b5f223bea4d418026662ffff08eab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59e1b7ae8f2b4558b24e73aca4e956d1","placeholder":"​","style":"IPY_MODEL_e913ecd6cb7746d4956a62ffdb0611cd","value":" 570/570 [00:00&lt;00:00, 17.3kB/s]"}},"fefbe13782c746e591ca915b345878c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"719c6dd822e1478faa48fd829c594ee6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75371e2e728b4d91a907b144bffe26b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a258401e6172493b9cabca14075bcd69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f38f337b3f54d7bb7c4d8035eb444f7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"59e1b7ae8f2b4558b24e73aca4e956d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e913ecd6cb7746d4956a62ffdb0611cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b1494645f6b480ba12f54570950369a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_025edcd2e178424d8da467d85f2a7216","IPY_MODEL_8f13baa0e7304e61a35555e2d2e82a2c","IPY_MODEL_f8ed2eaa2b194d0b89e2dddfb103c2c6"],"layout":"IPY_MODEL_22239a8d4a8145a9bd1a795e01125775"}},"025edcd2e178424d8da467d85f2a7216":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0131b73fc33c499fa2fe9bafb6e56260","placeholder":"​","style":"IPY_MODEL_81e7364e8e0e4ff9890b588b53c5d59f","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"8f13baa0e7304e61a35555e2d2e82a2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6f587a610bf47a2a64e1c98c02b5fc1","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a8aa9d8e559e466c8193d159abcc4ea0","value":231508}},"f8ed2eaa2b194d0b89e2dddfb103c2c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aac4a74fa8704cacb71408a8b7f2c6da","placeholder":"​","style":"IPY_MODEL_3dff8a2e59d142b280bf5ef7f8a12e3b","value":" 232k/232k [00:00&lt;00:00, 316kB/s]"}},"22239a8d4a8145a9bd1a795e01125775":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0131b73fc33c499fa2fe9bafb6e56260":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81e7364e8e0e4ff9890b588b53c5d59f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a6f587a610bf47a2a64e1c98c02b5fc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8aa9d8e559e466c8193d159abcc4ea0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aac4a74fa8704cacb71408a8b7f2c6da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dff8a2e59d142b280bf5ef7f8a12e3b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"47c3f2b20c92412c88b700f93d072a09":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f31cba4c582d4175bdb9059f226e478e","IPY_MODEL_ce97dfa293d74bc7ac57119ca24894cf","IPY_MODEL_4781d223ffa94593b18fc9265a360692"],"layout":"IPY_MODEL_c3cec1989d8a478aa6cf93750d243414"}},"f31cba4c582d4175bdb9059f226e478e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea48a130c782499c97761f49f4db13a7","placeholder":"​","style":"IPY_MODEL_1881ba54002f4ede9110f806992f9c88","value":"Downloading (…)/main/tokenizer.json: 100%"}},"ce97dfa293d74bc7ac57119ca24894cf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53ff917ffe184d2c9f5f047421fec1fb","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_eb4b737efc544fedb42984a2003ec87c","value":466062}},"4781d223ffa94593b18fc9265a360692":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3713ec5109be4d9fbfadc564aabfde31","placeholder":"​","style":"IPY_MODEL_bd7c372e411a4ab49fd576daf11f23ff","value":" 466k/466k [00:00&lt;00:00, 509kB/s]"}},"c3cec1989d8a478aa6cf93750d243414":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea48a130c782499c97761f49f4db13a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1881ba54002f4ede9110f806992f9c88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53ff917ffe184d2c9f5f047421fec1fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb4b737efc544fedb42984a2003ec87c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3713ec5109be4d9fbfadc564aabfde31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7c372e411a4ab49fd576daf11f23ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"da3cd0eb103d4a688a5e6fe8b7a5a8b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6ce477a96341412daa4604df798f52ba","IPY_MODEL_e0e2d7f651354fdebad07b1707b66ec3","IPY_MODEL_84da28dbe8cb4af1b8343cd8d185fc4f"],"layout":"IPY_MODEL_8fcb6862e05a4f2cb45ac54433246f9b"}},"6ce477a96341412daa4604df798f52ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_00fe3289aeaa4e6bb738d244ccc72c71","placeholder":"​","style":"IPY_MODEL_ef70c26fd1534ed496cae6941e8a11d7","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"e0e2d7f651354fdebad07b1707b66ec3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d955a572b5fe4da1979eb1ac8062d778","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2f39469fcb5148cb89b30cd22beb5768","value":440473133}},"84da28dbe8cb4af1b8343cd8d185fc4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7e6ba5949fc4dae947fec421389e746","placeholder":"​","style":"IPY_MODEL_f966c3efe2004d48a0a7d0ff468bc569","value":" 440M/440M [00:04&lt;00:00, 91.4MB/s]"}},"8fcb6862e05a4f2cb45ac54433246f9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00fe3289aeaa4e6bb738d244ccc72c71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef70c26fd1534ed496cae6941e8a11d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d955a572b5fe4da1979eb1ac8062d778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2f39469fcb5148cb89b30cd22beb5768":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7e6ba5949fc4dae947fec421389e746":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f966c3efe2004d48a0a7d0ff468bc569":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install bertviz"],"metadata":{"id":"jy2dQg8n_m4G"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This chapter dives deep into the Transformer encoder/decoder architectures. It starts by explaining encode model with a simple implementation of the attention. Then, multihead attention is given followed by the positional embeddings, layer normalization and skip connections. Combining all, we obtain a Transformer Encoder. Then the differences between encoder and decoder given as the masking and the attention over encoders' vectors. The chapter concludes by giving the sota models and the classification of the architectures. \n","\n","\n","* Dense vs sparse \n","* positional embeddign: absolute vs learnable, relative "],"metadata":{"id":"rpe3y3XPB0l-"}},{"cell_type":"markdown","source":["# Transformer Architecture \n","\n","Encoder-decoder models takes a sequence of words and generates another sequence of words related to the task like machine translation, summarization etc. \n","\n","The encoder converts the input sequences into a sequence of embeddings, called hidden state. The decoder takes the hidden states from the encoder and generates the output sequence. Both of them have a layered architecture. \n","\n","The models can be divided into three groups depending on their architecture:\n","* Encoder-only: These models converts the given input sequence into representations. They are mostly used for text classification, NER. BERT-like models.\n","* Decoder-only: These models generates a sequence of words when given an uncomplete start. GPT-like models. \n","* Encoder-decoder: These models are suitable for machine translation, summarization. BART and T5 models. \n","\n"],"metadata":{"id":"G1MrEIc83VJy"}},{"cell_type":"markdown","source":["## Encoder \n","\n","Each encoder block consists of a multi-head self-attention followed by a feed forward network. The output of the each encoder is the same size as the input of the encoder. \n","\n","* The role of the encoder block is to update the input embeddings to produce representations that encodes contextual information in the sequence. \n","\n","### Self-attention \n","\n","The \"self\" part refers to that the weights are computed for all hidden states in the same set. \n","\n","The idea is that instead of using a fixed embedding for each token, we can use the whole sequence to computed a weighted average of each embedding. \n","\n","**Formulation:** For the token embeddings are ${x_1,...,x_n}$ , the new embeddings are $x_{i}'= \\sum_{j=1}^n w_{ji}x_j$ where $w_{ji}$ are called the attention weights and normalized to have $\\sum_{j}w_{ji}=1$. \n","\n","These new embeddings are called *contextualized embeddings*.  \n","\n","**Scaled dot-product attention**\n","\n","The steps to compute the attention:\n","1. For each token embedding, make a projection over three vectors called query, key and value. \n","2. Compute the attention scores between query and key vectors. The attention scores show the similarity between these two vectors by computing dot product between the vectors. The output attention scores are $n \\times n$ matrix for a sequence with $n$ tokens. \n","\n","3. Compute attention weights by first multiplying the attention scores with a scaling factor, then normalizing with a softmax function. The resulting matrix ($n \\times n$) contains the attention weights $w_{ji}$. \n","\n","4. Multiply the value vector with the weights to obtain the updated token embeddings:  $x_{i}' = \\sum_{j}w_{ji}v_j$.\n"],"metadata":{"id":"mBGJfICz3VLV"}},{"cell_type":"code","source":["# Visualize the attentions \n","from transformers import AutoTokenizer\n","from bertviz.transformers_neuron_view import BertModel\n","from bertviz.neuron_view import show\n","\n","model_ckpt = \"bert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n","model = BertModel.from_pretrained(model_ckpt)\n","text = \"time flies like an arrow\"\n","show(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":611,"referenced_widgets":["9fb54453e24245e3a0d902fc26893588","6600b2b62c8542548e869b21d4662ce3","9d8e8302650c4607958f1259bcb377d7","c83ba7a776544b43afcf56411c6b70df","c28421b41fc2401ab61765d8d5c51b4b","2e33e1d5881a4eddbd1174ed425094d9","517839cfb1c84f65abd85c84aa3885f3","4a0128bb590c422cb9bec0a444ad45f9","8f89d23bcd3f449499448a1401774272","1011359bad5b4767b75dd6d2a1fad09e","5a4bd41668e644a591f82c2e8b7b0eee","3f4eb1ad73764165a5683cd9ec025fd7","9e5f0e03a8c84605ad8ecc470573163b","aaba5645fb9143d58d74dfd89bec41a0","560b5f223bea4d418026662ffff08eab","fefbe13782c746e591ca915b345878c3","719c6dd822e1478faa48fd829c594ee6","75371e2e728b4d91a907b144bffe26b8","a258401e6172493b9cabca14075bcd69","8f38f337b3f54d7bb7c4d8035eb444f7","59e1b7ae8f2b4558b24e73aca4e956d1","e913ecd6cb7746d4956a62ffdb0611cd","8b1494645f6b480ba12f54570950369a","025edcd2e178424d8da467d85f2a7216","8f13baa0e7304e61a35555e2d2e82a2c","f8ed2eaa2b194d0b89e2dddfb103c2c6","22239a8d4a8145a9bd1a795e01125775","0131b73fc33c499fa2fe9bafb6e56260","81e7364e8e0e4ff9890b588b53c5d59f","a6f587a610bf47a2a64e1c98c02b5fc1","a8aa9d8e559e466c8193d159abcc4ea0","aac4a74fa8704cacb71408a8b7f2c6da","3dff8a2e59d142b280bf5ef7f8a12e3b","47c3f2b20c92412c88b700f93d072a09","f31cba4c582d4175bdb9059f226e478e","ce97dfa293d74bc7ac57119ca24894cf","4781d223ffa94593b18fc9265a360692","c3cec1989d8a478aa6cf93750d243414","ea48a130c782499c97761f49f4db13a7","1881ba54002f4ede9110f806992f9c88","53ff917ffe184d2c9f5f047421fec1fb","eb4b737efc544fedb42984a2003ec87c","3713ec5109be4d9fbfadc564aabfde31","bd7c372e411a4ab49fd576daf11f23ff"],"output_embedded_package_id":"12mQ7vVwixw1KV12DWoqP_vqK2reKKN00"},"id":"AgkXndAt7oxt","executionInfo":{"status":"ok","timestamp":1675372878256,"user_tz":480,"elapsed":71053,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"27d7b3a8-a6b8-40c8-8d4f-801e7ec8eab7"},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["# First tokenize the text\n","# [CLS] and [SEP] tokens are expluded with add_special_tokens=False \n","inputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n","print(inputs.input_ids)\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_TnVQrbAECx","executionInfo":{"status":"ok","timestamp":1675373241069,"user_tz":480,"elapsed":322,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"42f49c67-14b5-4ec6-bcab-7a932386529c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2051, 10029,  2066,  2019,  8612]])\n","time flies like an arrow\n"]}]},{"cell_type":"code","source":["# Create a dense embedding matrix for the model's vocabulary\n","from torch import nn\n","from transformers import AutoConfig\n","\n","config = AutoConfig.from_pretrained(model_ckpt)\n","token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n","token_emb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TzG4NJTTBiQU","executionInfo":{"status":"ok","timestamp":1675373329481,"user_tz":480,"elapsed":1117,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"a3d8b24b-a2b2-40c0-80e4-b954aa721869"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(30522, 768)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# Get the embeddings for the inputs \n","# The resulting tensor [batch_size, seq_len, hidden_dim]\n","inputs_embeds = token_emb(inputs.input_ids)\n","inputs_embeds.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hPjElnRdCDOp","executionInfo":{"status":"ok","timestamp":1675373412541,"user_tz":480,"elapsed":5,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"b4da6471-2405-4031-9644-67f9161fa4f0"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# For now, we postpone the positional embeddings \n","# Create key, value and query matrices \n","import torch\n","from math import sqrt\n","\n","query = key = value = inputs_embeds\n","dim_k = key.size(-1)\n","print(\"Dimension is \", dim_k)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnqgNJO1CSh_","executionInfo":{"status":"ok","timestamp":1675373474312,"user_tz":480,"elapsed":376,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"3a01b7cd-3e9a-4ef9-ee82-70754a1eb9d1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimension is  768\n"]}]},{"cell_type":"markdown","source":["`torch.bmm()` performs a batch matrix-matrix product for inputs of shape `[batch_size, seq_len, hidden_dim]`. "],"metadata":{"id":"4Bg9ApQNDNdH"}},{"cell_type":"code","source":["print(\"Calculating the attention score with a dot-product between key and query\")\n","scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n","scores.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUyGWggMCm7d","executionInfo":{"status":"ok","timestamp":1675373508745,"user_tz":480,"elapsed":366,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"264b78d7-5508-4333-c618-b96774a63757"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Calculating the attention score with a dot-product between key and query\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 5])"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Now apply the softmax over the scaled dot-product \n","import torch.nn.functional as F\n","\n","weights = F.softmax(scores, dim=-1)\n","print(\"Sum of the wieghts \" , weights.sum(dim=-1))\n","print(\"Shape of the weights \", weights.shape)\n","\n","# Calculate the weighted vectors \n","attn_outputs = torch.bmm(weights, value)\n","print(\"Shape of weighted vectors \", attn_outputs.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdNksrUxDqVW","executionInfo":{"status":"ok","timestamp":1675373854064,"user_tz":480,"elapsed":336,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"04904313-d492-43d7-c9c4-8a1016def93e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Sum of the wieghts  tensor([[1., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n","Shape of the weights  torch.Size([1, 5, 5])\n","Shape of weighted vectors  torch.Size([1, 5, 768])\n"]}]},{"cell_type":"markdown","source":["In summary, self-attention contains two matrix multiplication and a softmax function. It is simply a form of averaging. \n"],"metadata":{"id":"yTfp7dqCESfR"}},{"cell_type":"code","source":["# Forming functions to be used later \n","def scaled_dot_product_attention(query, key, value):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    weights = F.softmax(scores, dim=-1)\n","    return torch.bmm(weights, value)"],"metadata":{"id":"ercOZu_SCvTJ","executionInfo":{"status":"ok","timestamp":1675373988848,"user_tz":480,"elapsed":411,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["**Multi-head Attention**\n","\n","In practice, the self-attention layer applies three independent linear transformations ot each embedding to generate the query, key and value vectors. Each linear projection is called attention head, thus forming multi-head attention layer. Having several heads allow the model to focus on several aspects at once. \n"],"metadata":{"id":"bqa1T8IXFGL5"}},{"cell_type":"code","source":["# A single attention head \n","# head_dim: nbr of dim that we are projecting into \n","# For example, BERT has 12 attention heads \n","#      so head_dim in each is 768/12 = 64\n","class AttentionHead(nn.Module):\n","  def __init__(self, embed_dim, head_dim):\n","    super().__init__() \n","    self.q = nn.Linear(embed_dim, head_dim) \n","    self.k = nn.Linear(embed_dim, head_dim)\n","    self.v = nn.Linear(embed_dim, head_dim)\n","  def forward(self, hidden_state):\n","    attn_outputs = scaled_dot_product_attention(self.q(hidden_state), \n","                                                self.k(hidden_state),\n","                                                self.v(hidden_state))\n","    return attn_outputs"],"metadata":{"id":"NmciQPbOEkZ0","executionInfo":{"status":"ok","timestamp":1675374740973,"user_tz":480,"elapsed":327,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Create Multi-Head attention layer \n","class MultiHeadAttention(nn.Module):\n","  def __init__(self,config):\n","    super().__init__()\n","    embed_dim = config.hidden_size \n","    num_heads = config.num_attention_heads \n","    head_dim = embed_dim // num_heads \n","    self.heads = nn.ModuleList(\n","        [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n","    )\n","    self.output_linear = nn.Linear(embed_dim,embed_dim)\n","  def forward(self, hidden_state):\n","    x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n","    x = self.output_linear(x)\n","    return x"],"metadata":{"id":"xsLUH-RwGix5","executionInfo":{"status":"ok","timestamp":1675374741990,"user_tz":480,"elapsed":2,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# What does multi-head layer generate? \n","# config is downloaded for the BERT model \n","multihead_attn = MultiHeadAttention(config)\n","# input_embeds are the vectors of shape [batch_size, seq_len, embed_dim]\n","attn_output = multihead_attn(inputs_embeds)\n","# the output is also the same shape \n","attn_output.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xgNks6piGMZB","executionInfo":{"status":"ok","timestamp":1675374828315,"user_tz":480,"elapsed":6,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"0ec1384a-c4c8-4aed-e6a7-fd3d5b4eca1c"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["# Visualize attention from a pre-trained model \n","from bertviz import head_view\n","from transformers import AutoModel\n","\n","model = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n","\n","sentence_a = \"time flies like an arrow\"\n","sentence_b = \"fruit flies like a banana\"\n","\n","viz_inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt')\n","attention = model(**viz_inputs).attentions\n","sentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\n","tokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n","\n","head_view(attention, tokens, sentence_b_start, heads=[8])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":335,"referenced_widgets":["da3cd0eb103d4a688a5e6fe8b7a5a8b0","6ce477a96341412daa4604df798f52ba","e0e2d7f651354fdebad07b1707b66ec3","84da28dbe8cb4af1b8343cd8d185fc4f","8fcb6862e05a4f2cb45ac54433246f9b","00fe3289aeaa4e6bb738d244ccc72c71","ef70c26fd1534ed496cae6941e8a11d7","d955a572b5fe4da1979eb1ac8062d778","2f39469fcb5148cb89b30cd22beb5768","c7e6ba5949fc4dae947fec421389e746","f966c3efe2004d48a0a7d0ff468bc569"],"output_embedded_package_id":"1y7QPaRl2yL5C5ksD_ywBYdOk1hukldcx"},"id":"k-8U6TeyHmMJ","executionInfo":{"status":"ok","timestamp":1675374882709,"user_tz":480,"elapsed":13743,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"37b772b4-5598-40ac-b1a1-ef140594a18a"},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["### Feed-Forward Layer \n","\n","This sublayer is a two-layer fully connected NN, but instead of processing the whole embeddings as a single vector, it processes each embedding independently. It is also called as *position-wise feed-forward layer*. \n","\n","This is where the most scaling is applied. "],"metadata":{"id":"_2GFbE5eKdbY"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","  def __init__(self,config): \n","    super().__init__()\n","    self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n","    self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n","    self.gelu = nn.GELU()\n","    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","  def forward(self,x): \n","    x = self.linear_1(x)\n","    x = self.gelu(x)\n","    x = self.linear_2(x)\n","    x = self.dropout(x)\n","    return x "],"metadata":{"id":"YnMN9PgaH7Z8","executionInfo":{"status":"ok","timestamp":1675375897893,"user_tz":480,"elapsed":415,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["# Testing the feed forward function \n","feed_forward = FeedForward(config)\n","ff_outputs = feed_forward(attn_outputs)\n","ff_outputs.size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7JF2hpcaLpgQ","executionInfo":{"status":"ok","timestamp":1675375899166,"user_tz":480,"elapsed":3,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"28bdfb77-f794-4e6f-f694-44ceeb570b83"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["### Adding Layer Normalization\n","\n","Layer normalization normalizes each input in the batch to have zero mean and unity variance. \n","\n","Skip connections pass a tensor to the next layer of the model without processing and add it to the next layer. \n","\n","1. Post layer normalization: In the original transformer paper, the layer normalization is placed between the skip connections. This may cause the gradients to diverge with a learning rate warm-up result. \n","\n","2. Pre layer normalization: places the normalizaation within the span of the skip connections. This tend to be much more stable during training. \n"],"metadata":{"id":"txkpQyqeNciA"}},{"cell_type":"code","source":["# Final encoder block \n","class TransformerEncoderLayer(nn.Module):\n","  def __init__(self,config): \n","    super().__init__()\n","    self.layer_norm_1 = nn.LayerNorm(config.hidden_size) \n","    self.layer_norm_2 = nn.LayerNorm(config.hidden_size) \n","    self.attention = MultiHeadAttention(config)\n","    self.feed_forward = FeedForward(config) \n","  def forward(self, x):\n","    # apply layer normalization \n","    hidden_state = self.layer_norm_1(x)\n","    # apply attention with a skip connection \n","    x = x + self.attention(hidden_state) \n","    # apply feed-forward with a skip connection \n","    x = x + self.feed_forward(self.layer_norm_2(x))\n","    return x "],"metadata":{"id":"0xYB6jmtLrk1","executionInfo":{"status":"ok","timestamp":1675376545470,"user_tz":480,"elapsed":376,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# Test it with the embeddings \n","encoder_layer = TransformerEncoderLayer(config)\n","inputs_embeds.shape, encoder_layer(inputs_embeds).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxLrq3vCOQCO","executionInfo":{"status":"ok","timestamp":1675376583566,"user_tz":480,"elapsed":811,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"7e4b3ded-1276-4047-b196-56407b72ac36"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([1, 5, 768]), torch.Size([1, 5, 768]))"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# What happens in the layer normalization?\n","normalized_embeds = encoder_layer.layer_norm_1(inputs_embeds)\n","torch.sum(normalized_embeds,2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Ns1nZGlOd0H","executionInfo":{"status":"ok","timestamp":1675376799955,"user_tz":480,"elapsed":354,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"34d955d0-76d9-4238-ac27-f290cd36c394"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-3.8147e-06,  2.8610e-06, -9.5367e-07,  1.1444e-05,  1.9073e-06]],\n","       grad_fn=<SumBackward1>)"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["# How does skip connection work? \n","inputs_embeds = inputs_embeds + encoder_layer.attention(normalized_embeds)"],"metadata":{"id":"psJpuP5yPWAL","executionInfo":{"status":"ok","timestamp":1675376866598,"user_tz":480,"elapsed":406,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["inputs_embeds.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dic835bAOoZ6","executionInfo":{"status":"ok","timestamp":1675376872121,"user_tz":480,"elapsed":305,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"a127980f-96cd-4386-d8fd-63924ec9320e"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["### Positional Embeddings \n","\n","Until now, the implemented model works but it doesn't know about the positions of the tokens. The way to incorporate it into model is using positional embeddings. The idea is to augment the token embeddings with a position-dependent pattern of values arranged in a vector. \n","\n","One of the most popular way is to use a learnable pattern which works the same way as the token embeddings. \n","\n","Now, we cretate a custom Embeddings module that combines a token embedding layer that projects the input otkens with a positional embeddings that project the position ids. "],"metadata":{"id":"Gslh62xvQM5M"}},{"cell_type":"code","source":["# Creating an embedding \n","class Embeddings(nn.Module):\n","  def __init__(self, config): \n","    super().__init__()\n","    self.token_embeddings = nn.Embedding(config.vocab_size,\n","                                         config.hidden_size)\n","    self.position_embeddings = nn.Embedding(config.max_position_embeddings,\n","                                            config.hidden_size)\n","    self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n","    self.dropout = nn.Dropout()\n","  def forward(self, input_ids):\n","    # create position ids for input sequence \n","    seq_length = input_ids.size(1)\n","    position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n","    # create token and position embeddings \n","    token_embeddings = self.token_embeddings(input_ids)\n","    position_embeddings = self.position_embeddings(position_ids)\n","    # Combine token and position embeddings \n","    embeddings = token_embeddings + position_embeddings  \n","    embeddings = self.layer_norm(embeddings)\n","    embeddings = self.dropout(embeddings)\n","    return embeddings \n"],"metadata":{"id":"lAvyksRGOyVU","executionInfo":{"status":"ok","timestamp":1675377537664,"user_tz":480,"elapsed":308,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Testing the embedding layer \n","embedding_layer = Embeddings(config)\n","embedding_layer(inputs.input_ids).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nSc1PyQFR5CZ","executionInfo":{"status":"ok","timestamp":1675377539106,"user_tz":480,"elapsed":388,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"b9f09632-2225-420d-de3d-421ce1d05605"},"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":60}]},{"cell_type":"markdown","source":["Finally, we have a Transformer Encoder layer: "],"metadata":{"id":"dscpxVLnSS7g"}},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","  def __init__(self, config): \n","    super().__init__()\n","    self.embeddings = Embeddings(config)\n","    self.layers = nn.ModuleList([TransformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n","  def forward(self,x):\n","    x = self.embeddings(x)\n","    for layer in self.layers:\n","      x = layer(x)\n","    return x "],"metadata":{"id":"LQ5ASvDTR8Z1","executionInfo":{"status":"ok","timestamp":1675378437578,"user_tz":480,"elapsed":293,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["encoder = TransformerEncoder(config)\n","encoder(inputs.input_ids).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pZ7jVY5TVbmP","executionInfo":{"status":"ok","timestamp":1675378440605,"user_tz":480,"elapsed":1680,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"d1ecba2a-1e5f-45b8-f802-26e767dc2ae8"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 768])"]},"metadata":{},"execution_count":64}]},{"cell_type":"markdown","source":["### Adding a classification Head \n","\n","Until now, we have an encoder that generates hidden state for each input token. This is the body of the Transformer. To use the model for prediction, we need to add a task-specific head like a classification head. "],"metadata":{"id":"a1eGS61uVoFu"}},{"cell_type":"code","source":["class TransformerForSequenceClassification(nn.Module):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.encoder = TransformerEncoder(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, x):\n","        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n","        x = self.dropout(x)\n","        x = self.classifier(x)\n","        return x"],"metadata":{"id":"9rrD1DIbVdfV","executionInfo":{"status":"ok","timestamp":1675378617422,"user_tz":480,"elapsed":340,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["# Defining the number of classes in the task  \n","config.num_labels = 3\n","encoder_classifier = TransformerForSequenceClassification(config)\n","encoder_classifier(inputs.input_ids).size()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4SifWusOWIww","executionInfo":{"status":"ok","timestamp":1675378620937,"user_tz":480,"elapsed":3059,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"6b47b8a9-7c26-4e67-fcc1-7d591151cc3e"},"execution_count":67,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3])"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["For each input in the batch (1 in this example), we get unnnormalized logits defining each class (3 in the example). "],"metadata":{"id":"yGtbPSp2WTir"}},{"cell_type":"markdown","source":["## Decoder \n","\n","The main differences between decoder and encoder are: \n","\n","* Masked multi-head self-attention layer: It ensures that only past outputs are used during current token prediction by masking targets of the steps.  \n","\n","* Encoder-decoder attention layer: It is a multi-head attention over the output key and value vectors of the encoder stack with the intermediate representations of the decoder acting as the queries. "],"metadata":{"id":"iWm8GsUZWdW0"}},{"cell_type":"code","source":["# creating a mask\n","seq_len = inputs.input_ids.size(-1)\n","mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0)\n","mask[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pv3aj9r_WOo8","executionInfo":{"status":"ok","timestamp":1675379374032,"user_tz":480,"elapsed":1362,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"82c3dbec-34b8-4f23-8e6f-00a943b6c2fd"},"execution_count":68,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0., 0., 0.],\n","        [1., 1., 0., 0., 0.],\n","        [1., 1., 1., 0., 0.],\n","        [1., 1., 1., 1., 0.],\n","        [1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["scores.masked_fill(mask == 0, -float(\"inf\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iE3nAeVoZHDB","executionInfo":{"status":"ok","timestamp":1675379389287,"user_tz":480,"elapsed":600,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}},"outputId":"0a64c15b-df69-4fca-dfeb-b7cdadc5d1e1"},"execution_count":69,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[28.8601,    -inf,    -inf,    -inf,    -inf],\n","         [ 0.1249, 25.3669,    -inf,    -inf,    -inf],\n","         [-1.4536,  0.7622, 26.2898,    -inf,    -inf],\n","         [-1.2275, -0.7349,  0.2297, 28.1567,    -inf],\n","         [ 0.8334, -0.5028,  0.3620,  0.3227, 27.6891]]],\n","       grad_fn=<MaskedFillBackward0>)"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["# New dot product with mask variable\n","def scaled_dot_product_attention(query, key, value, mask=None):\n","    dim_k = query.size(-1)\n","    scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n","    weights = F.softmax(scores, dim=-1)\n","    return weights.bmm(value)"],"metadata":{"id":"pPTvCf8bZKw8","executionInfo":{"status":"ok","timestamp":1675379415612,"user_tz":480,"elapsed":447,"user":{"displayName":"Merve Unlu Menevse","userId":"07208082643217875039"}}},"execution_count":70,"outputs":[]}]}